i---
title: "MAP Calculation"
author: "Arezoo Rafieeinasab"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  ---
  # Background
  
  
# Mean Areal Calculation

Often times, the mean areal precipitation is desired to be compared with the flooding events at the same time. Here we have compiled a set of functions and scripts which help you to calculate the Mean Areal from any gridded data (focus here is precipitation, but could be also used for snow or other components of the model). We explain the steps of calculating the Mean Areal Precipitation for the NWM forcing data as well as Stage IV data that is used for verification purposes.

Load rwrfhydro package. 
  ```{r  results='hide', message=FALSE, warning=FALSE}
library(rwrfhydro)
```
The MAP calculation functions are written recently and may not be available in the rwrfhydro version you are using. In that case, please source the following file which has the necessary functions to calculate the Mean Areal values.

```{r}
source("~/wrfHydroTestCases/04233300/Calc_Mean_Areal.R")
```

The following steps are taken when calculating the MAP values. First the mean areal precipitation is calculated over each NHD catchment (there are about 2.7 million of those catchments in US). To do that, you only need the sptial weight file. For example, 

```{r eval = FALSE}
forcFile = "~/wrfHydroTestCases/04233300/FORCING/2013092121.LDASIN_DOMAIN1"
rain <- rwrfhydro::ncdump(forcFile, "RAINRATE", quiet = TRUE)
meanP <- CalcMeanAreal(spatialWeightFile, rain)
```

Then we need to aggregate these catchments to the contributing area of a given gage. To be computationally efficient, we first divide the total area to what we call intervening areas. The intervening area for a given gage is all the NHD catchment between the desired gage and the immediate upstream gages. So they are the catchments with reaches draining to the desired gage without passing through any other gage. Then we calculate the mean areal precipitation over the contributing area. To perform this calculation, one need the list of all the upstream gages of a given gage, as well as the list of all the comIds for the intervening areas.

`GetUpStreamGages` return a list of all the existing gages in the Roulink with their upstream gages.

```{r eval = FALSE}
routlinkFile <- "~/wrfHydroTestCases/04233300/DOMAIN/RouteLink.nc"
upStreamGages <- GetUpStreamGages(routlinkFile)
```

`GetInterveningNHD` return a data.table of data.frame having the information on each gage and its corresponding comIds (within the intervening area of the gage) as well as the area of the catchment (WATCH! this is not actual area, this is number or fraction of pixels within a given NHDPlus catchment, for example for the case of NWM, 16 would be 1 sqkm since the routing grid are 250meter and the spatial weight file used here is for the overland flow routing grid (Fuldom grid))

```{r eval = FALSE}
spatialWeightFile <- "~/wrfHydroTestCases/04233300/DOMAIN/spatialweights.nc"
interveningNHD <- GetInterveningNHD(routlinkFile, spatialWeightFile)
```

`CalcMeanArealCont` function takes the `meanP` values above as well as the `upStreamGages` and `interveningNHD` data and return the mean areal precipitation for both the intervening and contributing area.

```{r eval = FALSE}
contP <- CalcMeanArealCont(meanArealDT = meanP, interveningNHD, upStreamGages)
```

You can loop over all the forcing files and calculate all the MAP values for the year of 2013. 

```{r eval = FALSE}
files <- list.files("~/wrfHydroTestCases/04233300/FORCING/", pattern = glob2rx("201308*"), full.names = TRUE)
library(foreach)
MAP <- foreach (f = files, .combine = rbind.data.frame) %do% {
  rain <- rwrfhydro::ncdump(f, "RAINRATE", quiet = TRUE)
  meanP <- CalcMeanAreal(spatialWeightFile, rain)
  contP <- CalcMeanArealCont(meanArealDT = meanP, interveningNHD, upStreamGages)
  contP$date <- as.POSIXct(basename(f), format = "%Y%m%d%H.LDASIN_DOMAIN1", tz = "UTC")
  return(contP)
}

library(ggplot2) 
ggplot(subset(MAP, gage == "04234000"), aes(date, meanContributing*3600)) + geom_bar(stat = 'identity') + scale_y_reverse()

```

Read in the streamflow simulations and plot them next to each other

```{r}
# list all the CHRTOUT files
files <- list.files("/glade/u/home/arezoo/wrfHydroTestCases/04233300/run.201306-201309.fullModel", pattern = glob2rx("2013*CHRTOUT_DOMAIN1"), full.names = TRUE)

# read the RoutLink file and extract the gages and link (comIDs) 
rtLink <- rwrfhydro::GetNcdfFile(routlinkFile, variables = c("gages", "link"), quiet = TRUE)
link <- subset(rtLink, trimws(gages) != "")$link

# read streamflow values for the gage locations
streamflow <- foreach (f = files, .combine = rbind.data.frame) %do% {
   streamflow <- rwrfhydro::GetNcdfFile(f, variables = c("station_id", "streamflow"), quiet = TRUE)
   DF <- data.frame(subset(streamflow, station_id %in% link))
   DF$date <-  as.POSIXct(basename(f), format = "%Y%m%d%H%M.CHRTOUT_DOMAIN1", tz = "UTC")
   return(DF)
}

# Add the gage info
streamflow <- merge(streamflow, rtLink, by.x = "station_id", by.y = "link")
```

Plot the MAP and streamflow values in two panels.

```{r}
streamflow$gages <- trimws(streamflow$gages)
merged <- subset(merge(streamflow, MAP, by.x = c("gages", "date"), by.y = c("gage", "date")), trimws(gages) == "04233300")

library(ggplot2)

g1 <- ggplot(merged, aes(date, meanContributing*3600)) + 
  geom_bar(stat = 'identity') + theme_bw() + ylab("MAP (mm)") + 
  ggtitle(paste0("USGS Station : ",  unique(merged$gage))) + scale_y_reverse() 

g2 <- ggplot(merged, aes(date,streamflow))+geom_line() + ylab("Streamflow (CMS)") + scale_y_continuous(trans="log10") + theme_bw() 

rwrfhydro::multiplot(g1,g2)
```

Often times we use hourly [StageIV](http://www.emc.ncep.noaa.gov/mmb/ylin/pcpanl/stage4/) for the verification of the precipitation product in NWM. However, hourly StageIV is poor in the western part of US, for that reason we use a blend of Stage IV and Stage II. The 6 hourly Stage IV (which has west RFC inputs) are disaggregated to hourly using the Stage II data as the weight. 
 
The Stage IV and Stage II data for the last two weeks are kept [online here](http://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/). Also, daily tar files are for the last few months are available [online here](http://ftp.emc.ncep.noaa.gov/mmb/precip/st2n4.arch/). And if a longer data record is required, the complete record is available on HPSS. 
* Stage II: /FS/DSS/DS507.5/ST2_4km; files named ST2_4km.YYYYMM.tar, January 2002 to October 2016.
* Stage IV: /FS/DSS/DS507.5/stage4; file named stage4.YYYYMM.tar, January 2002 to October 2016.

As explained above, due to the poor coverage of radar in the western part of US, the hourly Stage IV data are not high quality. However, the 6-hourly Stage IV data in the western side of US, are the RFC precipitation products based on the moutain mapper algorithm. The Stage II/IV job is run at 33min past the top of each hour. Hourly Stage IV is re-made hourly (if there is new input) for the first 23 hours after valid time, then again at 1/3/5/7 days after valid time. The 6-hourly Stage IV is re-made hourly (if there is new 6h input) for 42 hours after valid time; the four 6-hourlies covering a 12Z-12Z 24h period are also re-made on a fixed schedule, regardless of input, at 1/3/5/7 days after the ending 12Z (i.e. at the 12:33Z run); at which time the corresponding 24h totals are also re-made. Therefore, for blending, we go back in time and start blending files from 9 days ago to make sure we incorporate the latest changes in the files. 
 
We have provided the blended and regridded StaheIV/II data for the Sixmile Creek case study which can be used for verification. ....


